import time
import unicodedata
import websocket
import json
import requests
import re
import os
import mysql.connector
import threading
import queue
import multiprocessing
import urllib3  # ì¶”ê°€
import glob
import pdfplumber   # PDF ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import shutil       # íŒŒì¼ ì´ë™ì„ ìœ„í•œ ëª¨ë“ˆ
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from mysql.connector import pooling
from html import unescape
from pathlib import Path
from requests.exceptions import HTTPError
from collections import deque

# [ì¶”ê°€] SSL ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° (ë¡œê·¸ ê¹”ë”í•˜ê²Œ)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================
# 0. í™˜ê²½ ë³€ìˆ˜ ë° ê¸°ë³¸ ì„¤ì •
# ==========================================

load_dotenv()

n_cpu_cores = multiprocessing.cpu_count()

# í•„ìˆ˜ ê°’ í™•ì¸
if not os.getenv("DB_PASSWORD"):  # not os.getenv("LS_ACCESS_TOKEN") or
    # print("âŒ [Error] .env íŒŒì¼ì´ ì—†ê±°ë‚˜ í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜(TOKEN, PASSWORD)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("âŒ [Error] .env íŒŒì¼ì´ ì—†ê±°ë‚˜ DB_PASSWORDê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    exit(1)

# ==========================================
# 1. ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ (Configuration)
# ==========================================

WS_URL = "wss://openapi.ls-sec.co.kr:9443/websocket"
API_BASE_URL = "https://openapi.ls-sec.co.kr:8080"
# ACCESS_TOKEN = os.getenv("LS_ACCESS_TOKEN", "DUMMY_TOKEN")  # PDF ëª¨ë“œì¼ ê²½ìš° í† í° ì—†ì–´ë„ ì—ëŸ¬ ì•ˆ ë‚˜ê²Œ ì²˜ë¦¬
ACCESS_TOKEN = None  # ëŸ°íƒ€ì„ ë°œê¸‰/ê°±ì‹ 

# REST endpoint ìƒìˆ˜ (ì¤‘ë³µ ì œê±°)
# MARKET_DATA_EP = "/futureoption/market-data"
STOCK_INVESTINFO_EP = "/stock/investinfo"

# ==========================================
# [PATCH] LS OAuth2 Token Manager + Safe REST Wrapper
# ==========================================
APP_KEY = os.getenv("APP_KEY")
APP_SECRET = os.getenv("APP_SECRET")

TOKEN_URL = f"{API_BASE_URL}/oauth2/token"

_ACCESS_TOKEN = None
_ACCESS_TOKEN_EXPIRES_AT = 0  # epoch seconds

# [ì¤‘ìš”] ë¡œì»¬ Llama ì„œë²„ ì£¼ì†Œ ì„¤ì •
LLM_SERVER_URL = "http://localhost:8090/v1"

DB_CONFIG = {
    "host": os.getenv("DB_HOST", "localhost"),
    "user": os.getenv("DB_USER", "admin"),
    "password": os.getenv("DB_PASSWORD"),
    "database": os.getenv("DB_NAME", "LLM")
}

# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
PDF_BASE_PATH = Path(r"v:\Users\leeli\PycharmProjects\PythonProject1\data\pdf")

# ì‹œìŠ¤í…œ ë‚ ì§œë¥¼ ì½ì–´ì™€ ì‹¤ì œ ê°ì‹œí•  ê²½ë¡œ ìƒì„± (e.g. .../data/pdf/20251228)
current_date_str = datetime.now().strftime("%Y%m%d")
PDF_DIR_PATH = PDF_BASE_PATH / current_date_str

# news_queue = queue.Queue()
# í ì‚¬ì´ì¦ˆ ì œí•œ ì„¤ì • (ë©”ëª¨ë¦¬ í­ì£¼ ë°©ì§€)
# ë‰´ìŠ¤ê°€ ë„ˆë¬´ ë§ì´ ìŒ“ì´ë©´ ì›Œì»¤ê°€ ì²˜ë¦¬í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì œí•œì„ ë‘¡ë‹ˆë‹¤.
MAX_QUEUE_SIZE = 30
news_queue = queue.Queue(maxsize=MAX_QUEUE_SIZE)

# DB Pool ìƒì„±
try:
    db_pool = pooling.MySQLConnectionPool(
        pool_name="news_pool",
        pool_size=3,
        pool_reset_session=True,
        **DB_CONFIG
    )
    print("âœ… [System] DB Connection Pool ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"âŒ [System] DB Pool ìƒì„± ì‹¤íŒ¨: {e}")
    exit(1)

# ==========================================
# [PATCH] Context-safe ìš”ì•½ ìœ í‹¸ (8k/16kì—ì„œë„ ì•ˆ í„°ì§€ê²Œ)
# - í† í¬ë‚˜ì´ì € ì—†ì´ë„ ì•ˆì „í•˜ê²Œ ë™ì‘í•˜ë„ë¡ "ë¬¸ì ê¸¸ì´" ê¸°ì¤€ìœ¼ë¡œ ìƒí•œì„ ë‘ 
# - ìµœì¢… ìš”ì•½ ì…ë ¥ì„ í•­ìƒ ì œí•œ + ë‹¨ê³„ì (ê³„ì¸µ) ì¶•ì•½
# ==========================================

# ëŒ€ëµì ì¸ í† í° ì¶”ì •(í•œêµ­ì–´/ì˜ì–´ í˜¼í•© í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ ì¡ê¸° ìœ„í•´ ë³´ìˆ˜ì ìœ¼ë¡œ)
# ê²½í—˜ì ìœ¼ë¡œ "1 í† í° ~= 3~4 chars"ê°€ ë§ì§€ë§Œ, ì•ˆì „í•˜ê²Œ 3 chars/tokenë¡œ ê°€ì •
_CHARS_PER_TOKEN_EST = 3

# ì„œë²„ ì»¨í…ìŠ¤íŠ¸(n_ctx)ì— ë”°ë¼ ì¡°ì • (8k/16k ë‘˜ ë‹¤ ì•ˆì „í•˜ê²Œ)
# - 8k ê¸°ì¤€: ì…ë ¥(í”„ë¡¬í”„íŠ¸) 16k chars ì •ë„ë©´ ëŒ€ì²´ë¡œ ì•ˆì „
# - 16k ê¸°ì¤€: ì…ë ¥ 32k chars ì •ë„ ê°€ëŠ¥
# ì—¬ê¸°ì„œëŠ” "ì ˆëŒ€ ì•ˆ í„°ì§€ê²Œ"ê°€ ëª©í‘œì´ë¯€ë¡œ ë³´ìˆ˜ì ìœ¼ë¡œ ë‚®ì¶¤
LLM_MAX_USER_CHARS = 12000      # user_prompt ìµœëŒ€ ê¸¸ì´(í•˜ë“œ ì»·)
LLM_MAX_COMBINED_CHARS = 9000  # chunk summary í•©ì¹œ combined ìµœëŒ€ ê¸¸ì´
LLM_MAX_CHUNK_SUMMARIES = 14    # 1ì°¨ ì²­í¬ ìš”ì•½ ìµœëŒ€ ê°œìˆ˜(ë„ˆë¬´ ë§ìœ¼ë©´ ê³„ì¸µ ì¶•ì•½)

def _safe_truncate(text: str, max_chars: int) -> str:
    if not text:
        return ""
    if len(text) <= max_chars:
        return text
    head = text[: int(max_chars * 0.88)]
    tail = text[-int(max_chars * 0.07):] if max_chars >= 600 else ""
    return (head + "\n...\n" + tail).strip()

def _build_user_prompt(prefix: str, content: str, max_chars: int = LLM_MAX_USER_CHARS) -> str:
    return _safe_truncate(f"{prefix}\n{content}".strip(), max_chars)

def call_llm_api(system_prompt, user_prompt, max_tokens=2048):
    """
    OpenAI ìŠ¤íƒ€ì¼ API í˜¸ì¶œ í—¬í¼ í•¨ìˆ˜ (ì»¨í…ìŠ¤íŠ¸ ì´ˆê³¼ ë°©ì§€)
    - user_prompt í•˜ë“œ ì»·
    - ì»¨í…ìŠ¤íŠ¸ ì´ˆê³¼ ì—ëŸ¬ë©´ ë” ì¤„ì—¬ 1íšŒ ì¬ì‹œë„
    """
    user_prompt = _safe_truncate(user_prompt or "", LLM_MAX_USER_CHARS)

    try:
        with llm_lock:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=max_tokens,
                stream=False,
                timeout=300
            )
            return response.choices[0].message.content.strip()

    except Exception as e:
        msg = str(e)
        print(f"âš ï¸ [LLM API Error] ì„œë²„ í†µì‹  ì‹¤íŒ¨: {e}")
        print(f"   - user_prompt length: {len(user_prompt) if user_prompt else 0}")

        if ("exceeds the available context size" in msg) or ("exceed_context_size" in msg) or ("n_ctx" in msg):
            shorter = _safe_truncate(user_prompt, 6000)
            print(f"ğŸ”§ [LLM Retry] ì»¨í…ìŠ¤íŠ¸ ì´ˆê³¼ -> prompt ì¶•ì†Œ í›„ ì¬ì‹œë„ ({len(shorter)} chars)")
            try:
                with llm_lock:
                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": shorter}
                        ],
                        temperature=0.1,
                        max_tokens=max_tokens,
                        stream=False,
                        timeout=120
                    )
                    return response.choices[0].message.content.strip()
            except Exception as e2:
                print(f"âš ï¸ [LLM API Error] ì¬ì‹œë„ë„ ì‹¤íŒ¨: {e2}")
                return ""

        return ""

# ==========================================
# 2. LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (OpenAI í˜¸í™˜)
# ==========================================

print(f"â³ [System] ë¡œì»¬ LLM ì„œë²„({LLM_SERVER_URL})ì— ì—°ê²° ì„¤ì • ì¤‘...")

try:
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    client = OpenAI(
        base_url=LLM_SERVER_URL,
        api_key="no-key-needed"  # ë¡œì»¬ ì„œë²„ë¼ í‚¤ëŠ” ì•„ë¬´ê±°ë‚˜ ì…ë ¥
    )
    print("âœ… [System] LLM í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
except Exception as e:
    print(f"âš ï¸ [Warning] í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì¤‘ ì—ëŸ¬: {e}")

# LLM í˜¸ì¶œ ì§ë ¬í™”ë¥¼ ìœ„í•œ ì „ì—­ ë½ (ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€ìš©)
llm_lock = threading.Lock()

# í‚¤ì›Œë“œ ì •ì˜
from keywords import (
    MACRO_KEYWORDS,             # MACRO_KEYWORDS = ["ê¸ˆë¦¬", "í™˜ìœ¨", "CPI", "PPI", "FOMC", "ì—°ì¤€", "GDP", "ë¬¼ê°€", "ìœ ê°€"]
    GLOBAL_KEYWORDS,            # GLOBAL_KEYWORDS = ["ë‚˜ìŠ¤ë‹¥", "ë‹¤ìš°", "S&P500", "ë‰´ìš•ì¦ì‹œ", "í…ŒìŠ¬ë¼", "ì—”ë¹„ë””ì•„", "ì• í”Œ", "TSMC"]
    DOMESTIC_MARKET_KEYWORDS,   # DOMESTIC_MARKET_KEYWORDS = ["ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì™¸êµ­ì¸", "ê¸°ê´€", "ìˆœë§¤ìˆ˜", "ê³µë§¤ë„"]
    ETC_KEYWORDS,               # ETC_KEYWORDS = ["ë¹„íŠ¸ì½”ì¸", "ê°€ìƒí™”í", "IPO", "ê³µëª¨ì£¼"]
)

def issue_ls_access_token(force: bool = False) -> str:
    """
    LS OAuth2 Client Credentials í† í° ë°œê¸‰/ê°±ì‹ 
    - force=True: ë¬´ì¡°ê±´ ì¬ë°œê¸‰
    """
    global _ACCESS_TOKEN, _ACCESS_TOKEN_EXPIRES_AT

    now = int(time.time())

    # ë§Œë£Œ 60ì´ˆ ì „ì´ë©´ ë¯¸ë¦¬ ê°±ì‹ 
    if (not force) and _ACCESS_TOKEN and now < (_ACCESS_TOKEN_EXPIRES_AT - 60):
        return _ACCESS_TOKEN

    if not APP_KEY or not APP_SECRET:
        raise RuntimeError("âŒ APP_KEY/APP_SECRET(.env) ëˆ„ë½")

    headers = {"Content-Type": "application/x-www-form-urlencoded"}
    data = {
        "grant_type": "client_credentials",
        "appkey": APP_KEY,
        "appsecretkey": APP_SECRET,
        "scope": "oob",
    }

    resp = requests.post(TOKEN_URL, headers=headers, data=data, verify=False, timeout=5)

    if resp.status_code != 200:
        raise RuntimeError(f"âŒ [LS] í† í° ë°œê¸‰ ì‹¤íŒ¨: {resp.status_code} / {resp.text}")

    j = resp.json()
    token = j.get("access_token")
    expires_in = int(j.get("expires_in", 1800))  # ì—†ìœ¼ë©´ 30ë¶„ ê°€ì •

    if not token:
        raise RuntimeError(f"âŒ [LS] í† í° ì‘ë‹µì— access_token ì—†ìŒ: {j}")

    _ACCESS_TOKEN = token
    _ACCESS_TOKEN_EXPIRES_AT = now + expires_in
    print("âœ… [LS] Access Token ë°œê¸‰/ê°±ì‹  ì™„ë£Œ")

    return _ACCESS_TOKEN


def get_headers(tr_cd, tr_cont="N"):
    token = issue_ls_access_token(force=False)
    return {
        "Content-Type": "application/json; charset=UTF-8",
        "Authorization": f"Bearer {token}" if token else "Bearer ",
        "tr_cd": tr_cd,
        "tr_cont": tr_cont,
        "mac_address": "00:11:22:33:44:55"
    }


def ls_post(endpoint_path: str, tr_cd: str, body: dict, timeout: int = 5) -> dict:
    """
    LS REST ê³µí†µ POST
    - 401ì´ë©´ í† í° ê°•ì œ ì¬ë°œê¸‰ í›„ 1íšŒ ì¬ì‹œë„
    """
    url = f"{API_BASE_URL}{endpoint_path}"

    headers = get_headers(tr_cd)
    resp = requests.post(url, headers=headers, data=json.dumps(body), verify=False, timeout=timeout)

    if resp.status_code == 401:
        issue_ls_access_token(force=True)
        headers = get_headers(tr_cd)
        resp = requests.post(url, headers=headers, data=json.dumps(body), verify=False, timeout=timeout)

    resp.raise_for_status()
    return resp.json()

# ==========================================
# 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ì •ì œ, ë³‘í•©, DB)
# ==========================================

def get_safe_pdf_path(base_dir, date_str, file_name):
    """
    íŒŒì¼ ê²½ë¡œë¥¼ ì•ˆì „í•˜ê²Œ ìƒì„±í•˜ê³  ì¡´ì¬ ì—¬ë¶€ë¥¼ ì²´í¬í•˜ëŠ” í•¨ìˆ˜
    """
    # 1. Path ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ OSë³„ ê²½ë¡œ êµ¬ë¶„ì(/ vs \) ì´ìŠˆ ìë™ í•´ê²°
    # ê³µë°±ì´ë‚˜ í•œê¸€ì´ í¬í•¨ëœ íŒŒì¼ëª…ë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    base_path = Path(base_dir)
    full_path = base_path / "data" / "pdf" / date_str / file_name

    # 2. ìƒìœ„ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸
    if not full_path.parent.exists():
        print(f"âš ï¸ [System] ë””ë ‰í† ë¦¬ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {full_path.parent}")
        # í•„ìš” ì‹œ ë””ë ‰í† ë¦¬ ìƒì„± ë¡œì§ ì¶”ê°€
        # full_path.parent.mkdir(parents=True, exist_ok=True)
        return None

    # 3. íŒŒì¼ ìì²´ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not full_path.exists():
        print(f"âŒ [Errno 2] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")
        return None

    return full_path

def clean_financial_text(text: str) -> str:
    """ê¸ˆìœµ í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text: return ""
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r'^[A-Za-z0-9]+OutBlock\d+\s+', '', text)
    lines = text.splitlines()
    merged_lines = []

    bullet_pattern = re.compile(r'^[\*\-â€¢â€»\[]')
    finance_symbol_pattern = re.compile(r'^[â–²â–¼â–³â–½â†‘â†“]')
    starts_with_number = re.compile(r'^[â–²â–¼â–³â–½â†‘â†“]\s*[0-9\.]')

    for line in lines:
        line = line.strip()
        if not line: continue

        if not merged_lines:
            merged_lines.append(line)
            continue
        prev_line = merged_lines[-1]

        if bullet_pattern.match(line):
            merged_lines.append(line)
        elif finance_symbol_pattern.match(line):
            if starts_with_number.match(line):
                merged_lines[-1] += " " + line
            else:
                merged_lines.append(line)
        elif prev_line.endswith('.') or prev_line.endswith(':'):
            merged_lines.append(line)
        else:
            merged_lines[-1] += " " + line

    result = "\n".join(merged_lines)
    result = re.sub(r'(\n)([â–²â–¼â–³â–½â†‘â†“])(?!\s*[0-9])', r'\n\n\2', result)
    return result


def clean_base_text(text: str) -> str:
    """HTML ë° ë…¸ì´ì¦ˆ ì œê±°"""
    if not text or not isinstance(text, str):
        return ""
    text = unescape(text)
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r'<.*?>', '', text, flags=re.DOTALL)
    text = re.sub(r'(@media.*?\{.*?\})|(\{.*?\})', '', text, flags=re.DOTALL)
    text = re.sub(r'http[s]?://\S+', '', text)
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', text)
    text = re.sub(r'^[A-Za-z0-9]+OutBlock\d+\s+', '', text)
    text = re.sub(r'[^\w\s.,\'"()%\+/\-â–²â–¼â–³â–½â†‘â†“]', '', text)
    return text


def merge_news_bodies(news_bodies):
    """ë‰´ìŠ¤ ë³¸ë¬¸ ë°°ì—´ ë³‘í•©"""
    merged_lines = []
    for news in news_bodies:
        line = news['sBody'].strip()
        if not line:
            continue
        if not merged_lines:
            merged_lines.append(line)
            continue
        if merged_lines[-1].endswith('.') or merged_lines[-1].endswith(':'):
            merged_lines.append(line)
        else:
            merged_lines[-1] += " " + line
    return "\n".join(merged_lines)


def insert_to_db(data):
    """MySQL ë°ì´í„° ì €ì¥"""
    conn = None
    try:
        conn = db_pool.get_connection()
        # [ì¶”ê°€] ì—°ê²°ì´ ìœ íš¨í•œì§€ í™•ì¸í•˜ê³  í•„ìš”í•˜ë©´ ì¬ì—°ê²°
        if not conn.is_connected():
            conn.reconnect(attempts=3, delay=1)

        if conn.is_connected():
            cursor = conn.cursor()
            # realkeyê°€ PDFì¸ ê²½ìš° íŒŒì¼ëª… ë“±ìœ¼ë¡œ ëŒ€ì²´ë¨
            insert_query = """
                           INSERT INTO news_data (date, time, id, realkey, title, bodysize, category, body)
                           VALUES (%s, %s, %s, %s, %s, %s, %s, %s);
                           """
            cursor.execute(insert_query, data)
            conn.commit()
    except mysql.connector.Error as err:
        print(f"âŒ DB ì—ëŸ¬: {err}")
    finally:
        if conn:
            conn.close()


# ==========================================
# 4. í‚¤ì›Œë“œ ê¸°ë°˜ 1ì°¨ ë¶„ë¥˜
# ==========================================

def quick_keyword_classify(title: str) -> str | None:
    t = title.strip()
    if any(k in t for k in MACRO_KEYWORDS): return "ê±°ì‹œê²½ì œ"

    if any(k in t for k in DOMESTIC_MARKET_KEYWORDS): return "êµ­ë‚´ ì‹œí™©"

    if any(k in t for k in GLOBAL_KEYWORDS): return "í•´ì™¸ ì¦ì‹œ"

    if any(k in t for k in ETC_KEYWORDS): return "ê¸°íƒ€"

    return None


# ==========================================
# 5. ìš”ì•½ + ë¶„ë¥˜ í†µí•© LLM í˜¸ì¶œ (OpenAI API ë°©ì‹)
# ==========================================
#
# def call_llm_api(system_prompt, user_prompt, max_tokens=1024):
#     """
#     OpenAI ìŠ¤íƒ€ì¼ API í˜¸ì¶œ í—¬í¼ í•¨ìˆ˜
#     """
#     try:
#         # ìŠ¤ë ˆë“œ ë½ ì‚¬ìš© (ì„œë²„ì— ë™ì‹œ ìš”ì²­ì´ ë„ˆë¬´ ëª°ë¦¬ì§€ ì•Šë„ë¡ ì¡°ì ˆ)
#         with llm_lock:
#             response = client.chat.completions.create(
#                 model="gpt-3.5-turbo",  # ë¡œì»¬ ì„œë²„ì—ì„œëŠ” ëª¨ë¸ëª… ë¬´ì‹œë¨ (ì„œë²„ì— ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)
#                 messages=[
#                     {"role": "system", "content": system_prompt},
#                     {"role": "user", "content": user_prompt}
#                 ],
#                 temperature=0.1,
#                 max_tokens=max_tokens,
#                 stream=False,  # ìŠ¤íŠ¸ë¦¬ë° ë” (í•œ ë²ˆì— ë°›ê¸°)
#                 timeout = 120  # 60ì´ˆ ì§€ë‚˜ë©´ ì—ëŸ¬ ë°œìƒì‹œí‚¤ê³  ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°
#             )
#             return response.choices[0].message.content.strip()
#     except Exception as e:
#         print(f"âš ï¸ [LLM API Error] ì„œë²„ í†µì‹  ì‹¤íŒ¨: {e}")
#         print(f"   - user_prompt length: {len(user_prompt) if user_prompt else 0}")
#         return ""

def get_heuristic_summary(text: str, max_sentences: int = 3) -> str:
    """LLM ì‹¤íŒ¨ ì‹œ ì‘ë™í•˜ëŠ” ê·œì¹™ ê¸°ë°˜ ìš”ì•½ (ì²« ë¬¸ì¥ê³¼ ë§ˆì§€ë§‰ ë¬¸ì¥ ì¶”ì¶œ)"""
    if not text: return ""
    # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬ (ë‹¨ìˆœ ë§ˆì¹¨í‘œ ê¸°ì¤€)
    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', text) if s.strip()]
    if len(sentences) <= max_sentences:
        return text

    # ì• 2ë¬¸ì¥ + ë’¤ 1ë¬¸ì¥ ì¡°í•©
    head = " ".join(sentences[:2])
    tail = sentences[-1]
    return f"[ìë™ì¶”ì¶œ] {head} ... {tail}"

def summarize_and_classify(text: str, title: str) -> tuple[str, str]:
    """
    1) ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²­í¬ ë‹¨ìœ„ ìš”ì•½ í›„ ìµœì¢… ìš”ì•½
    2) ìµœì¢… ìš”ì•½ + ì œëª©ì„ ì´ìš©í•´ LLMìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
    """
    category = "ê¸°íƒ€_LLM"
    final_summary = "ë‚´ìš© ì—†ìŒ"

    if not text:
        return "ë‚´ìš© ì—†ìŒ", "ê¸°íƒ€"

    # 1ì°¨: í‚¤ì›Œë“œ ë¶„ë¥˜
    kw_category = quick_keyword_classify(title)

    # if kw_category in ["ê±°ì‹œê²½ì œ"]:
    # í…ìŠ¤íŠ¸ ë¶„í•  (LangChain Splitter í™œìš©)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)
    chunks = text_splitter.split_text(text)

    summary_system_prompt = (
        "ì—­í• : ë‹¹ì‹ ì€ íˆ¬ìì ê´€ì ì˜ ì „ë¬¸ ìš”ì•½ê°€ì´ë‹¤.\n"
        "ëª©í‘œ: ì‚¬ìš©ìê°€ ì œê³µí•œ ë‰´ìŠ¤/ê¸°ì‚¬/ë¦¬í¬íŠ¸/ê³µì§€ í…ìŠ¤íŠ¸ì—ì„œ íˆ¬ììê°€ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•´ ìš”ì•½í•œë‹¤.\n\n"
        "ì¶œë ¥ ê·œì¹™:\n"
        "- í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.\n"
        "- ì œëª©, ì„œë¡ , ì¸ì‚¬ë§, ëª©ë¡ í‘œê¸° ì—†ì´ ë³¸ë¬¸ìœ¼ë¡œ ë°”ë¡œ ì‹œì‘í•œë‹¤.\n"
        "- ì •í™•íˆ 3~5ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±í•œë‹¤.\n"
        "- ì•„ë˜ 4ê°€ì§€ ìš”ì†Œë¥¼ ê°€ëŠ¥í•œ í•œ í¬í•¨í•œë‹¤:\n"
        "  1) ì¤‘ìš”í•œ ìˆ˜ì¹˜/í†µê³„(ê¸ˆì•¡, ì„±ì¥ë¥ , ë§¤ì¶œ/ì´ìµ, ë¬¼ê°€, ê¸ˆë¦¬, ì ìœ ìœ¨ ë“±)\n"
        "  2) ê´€ë ¨ ê¸°ì—…ëª…/ê¸°ê´€/ì¸ë¬¼\n"
        "  3) ì£¼ìš” ì‚¬ê±´ê³¼ ì‹œì¥ ë™í–¥(ê·œì œ, ì¸ìˆ˜í•©ë³‘, ì‹¤ì , ì •ì±…, ìˆ˜ìš”/ê³µê¸‰ ë³€í™” ë“±)\n"
        "  4) ê²½ì œì  ì˜í–¥ ë° í–¥í›„ ì „ë§(í…ìŠ¤íŠ¸ì— ê·¼ê±°í•œ ë²”ìœ„ì—ì„œë§Œ)\n\n"
        "ì œì•½:\n"
        "- ë¶ˆí•„ìš”í•œ ë°°ê²½ ì„¤ëª…, ê°ì •ì  í‘œí˜„, ê³¼ë„í•œ ì„¸ë¶€ì‚¬í•­ì€ ì œì™¸í•œë‹¤.\n"
        "- ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ê±°ë‚˜ ë‹¨ì •í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
        "- ë°ì´í„°ê°€ ë¶ˆëª…í™•í•˜ë©´ 'ì›ë¬¸ì— ìˆ˜ì¹˜/ì •ë³´ ì—†ìŒ'ì²˜ëŸ¼ ê°„ë‹¨íˆ ëª…ì‹œí•œë‹¤.\n"
    )

    final_system_prompt = (
        "ì—­í• : ë‹¹ì‹ ì€ íˆ¬ìì ê´€ì ì˜ ìµœì¢… ìš”ì•½ í¸ì§‘ìì´ë‹¤.\n"
        "ëª©í‘œ: ì•„ë˜ 'ë¶€ë¶„ ìš”ì•½ë“¤'ì„ ì½ê³ , ì¤‘ë³µì„ ì œê±°í•œ ë’¤ í•µì‹¬ë§Œ ë‚¨ê²¨ ìµœì¢… ìš”ì•½ì„ ë§Œë“ ë‹¤.\n"
        "ì¶œë ¥ ê·œì¹™:\n"
        "- í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.\n"
        "- ì •í™•íˆ 3~5ë¬¸ì¥.\n"
        "- ìˆ«ì/ê¸°ì—…/ê¸°ê´€/ì •ì±…/ì‹œì¥ì˜í–¥ì„ ìš°ì„ í•œë‹¤.\n"
        "- ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
    )

    def _llm_summary_single(chunk_text: str) -> str:
        # ìƒˆë¡œ ë§Œë“  API í˜¸ì¶œ í•¨ìˆ˜ ì‚¬ìš©
        # result = call_llm_api(summary_system_prompt, f"ë‰´ìŠ¤ ë³¸ë¬¸:\n{chunk_text}")
        user_prompt = _build_user_prompt("ë‰´ìŠ¤ ë³¸ë¬¸:", chunk_text, max_chars=LLM_MAX_USER_CHARS)
        result = call_llm_api(summary_system_prompt, user_prompt, max_tokens=2048)

        # ë¶ˆí•„ìš”í•œ ì„œë‘ ì œê±°
        result = re.sub(r"^(\s*ìš”ì•½\s*[:\-\]]?|.*?ìš”ì•½í•´\s*ë“œë¦¬ê² ìŠµë‹ˆë‹¤[.]?)", "", result).strip()
        return result

    def _llm_final(partials: str) -> str:
        partials = _safe_truncate(partials, LLM_MAX_COMBINED_CHARS)
        user_prompt = _build_user_prompt("ë¶€ë¶„ ìš”ì•½ë“¤:", partials, max_chars=LLM_MAX_USER_CHARS)
        result = call_llm_api(final_system_prompt, user_prompt, max_tokens=2048)
        result = re.sub(r"^(\s*ìš”ì•½\s*[:\-\]]?|.*?ìš”ì•½í•´\s*ë“œë¦¬ê² ìŠµë‹ˆë‹¤[.]?)", "", result).strip()
        return result

    def _hier_reduce(summaries: list[str]) -> str:
        summaries = [s for s in summaries if s and s.strip()]
        if not summaries:
            return ""

        q = deque(summaries)
        BATCH_SIZE = 5

        while True:
            if len(q) == 1:
                return q[0]

            reduced = []
            while q:
                batch = []
                while q and len(batch) < BATCH_SIZE:
                    batch.append(q.popleft())
                out = _llm_final("\n".join(batch))
                if out:
                    reduced.append(out)

            if not reduced:
                return ""
            q = deque(reduced)

    # 1) ì²­í¬ ìš”ì•½
    if len(chunks) == 1:
        print(f"ğŸ§© [System] í…ìŠ¤íŠ¸ 1ê°œì˜ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        # final_summary = _llm_summary_single(text)
        final_summary = _llm_summary_single(_safe_truncate(text, LLM_MAX_USER_CHARS))
    else:
        print(f"ğŸ§© [System] ê¸´ í…ìŠ¤íŠ¸ ë¶„í•  ì²˜ë¦¬ ({len(chunks)}ê°œ)")
        chunk_summaries = []
        # for i, chunk in enumerate(chunks):
        #     print(f"   ... {i + 1}/{len(chunks)} ë²ˆì§¸ ì²­í¬ ìš”ì•½ ì¤‘")
        # ì²­í¬ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì•/ë’¤ ì¼ë¶€ë§Œ 1ì°¨ ìš”ì•½ (ì»¨í…ìŠ¤íŠ¸/ì‹œê°„ ë³´í˜¸)
        if len(chunks) > LLM_MAX_CHUNK_SUMMARIES:
            head_n = LLM_MAX_CHUNK_SUMMARIES // 2
            tail_n = LLM_MAX_CHUNK_SUMMARIES - head_n
            selected = chunks[:head_n] + chunks[-tail_n:]
            print(f"ğŸ§¯ [SAFE] ì²­í¬ ê³¼ë‹¤({len(chunks)}ê°œ) -> {len(selected)}ê°œ(ì•{head_n}+ë’¤{tail_n})ë§Œ 1ì°¨ ìš”ì•½")
        else:
            selected = chunks

        for i, chunk in enumerate(selected):
            print(f"   ... {i + 1}/{len(selected)} ë²ˆì§¸ ì²­í¬ ìš”ì•½ ì¤‘")
            summary = _llm_summary_single(chunk)
            chunk_summaries.append(summary)

        # combined = "\n".join(chunk_summaries)
        #
        # if len(combined) > 1000:
        #     print("ğŸ [System] ìµœì¢… ìš”ì•½ë³¸ ìƒì„± ì¤‘...")
        #     final_summary = _llm_summary_single(combined)
        # else:
        #     print("ğŸ [System] ìµœì¢… ìš”ì•½ë³¸ ìƒì„± ...")
        #     final_summary = combined
        combined = "\n".join([s for s in chunk_summaries if s and s.strip()])
        combined = _safe_truncate(combined, LLM_MAX_COMBINED_CHARS)

        # ìµœì¢… ìš”ì•½(ì»¨í…ìŠ¤íŠ¸ ì•ˆì „)
        print("ğŸ [System] ìµœì¢… ìš”ì•½ë³¸ ìƒì„± ì¤‘... (n_ctx=8192 ì•ˆì „ ëª¨ë“œ)")
        if len(chunk_summaries) > 5:
            final_summary = _hier_reduce(chunk_summaries)
        else:
            final_summary = _llm_final(combined)

        if not final_summary:
            final_summary = _safe_truncate(text, 1200)

    # 2) ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
    if kw_category is not None:
        category = kw_category + "_KEY"
    else:
        classify_system = (
            "ì—­í• : ë‹¹ì‹ ì€ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ë¥˜ê¸°ì´ë‹¤.\n"
            "ì„ë¬´: ì‚¬ìš©ìê°€ ì œê³µí•œ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ë¥¼ ì•„ë˜ 5ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•œë‹¤.\n\n"
            "ì¹´í…Œê³ ë¦¬ ì •ì˜(íƒ1):\n"
            "1) ê±°ì‹œê²½ì œ: ê¸ˆë¦¬, í™˜ìœ¨, ìœ ê°€, CPI/PPI, ê³ ìš©/ì„±ì¥ ì§€í‘œ, ì¤‘ì•™ì€í–‰(ì—°ì¤€/Fed ë“±), ê²½ê¸°/ì¸í”Œë ˆì´ì…˜.\n"
            "2) í•´ì™¸ ì¦ì‹œ: ë¯¸êµ­/í•´ì™¸ ì£¼ê°€ì§€ìˆ˜, í•´ì™¸ ì‹œì¥ ë™í–¥, í•´ì™¸ ìƒì¥ê¸°ì—…(ì˜ˆ: ì• í”Œ, ì—”ë¹„ë””ì•„ ë“±) ê´€ë ¨.\n"
            "3) êµ­ë‚´ ì‹œí™©: ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ ì§€ìˆ˜, êµ­ë‚´ ì¦ì‹œ ì „ë°˜, ì™¸êµ­ì¸/ê¸°ê´€/ê°œì¸ ìˆ˜ê¸‰, í”„ë¡œê·¸ë¨ ë§¤ë§¤.\n"
            "4) ì£¼ë„ ì„¹í„°: êµ­ë‚´ ê°œë³„ ê¸°ì—…(ì‹¤ì /ê³µì‹œ/ìˆ˜ì£¼/í•©ë³‘ ë“±) ë˜ëŠ” êµ­ë‚´ ì‚°ì—…/í…Œë§ˆ(ë°˜ë„ì²´, 2ì°¨ì „ì§€, ë°”ì´ì˜¤ ë“±).\n"
            "5) ê¸°íƒ€: ê°€ìƒìì‚°, ì •ì±…/ê·œì œ, IPO/ìƒì¥, ì§€ì •í•™/ì „ìŸ/ì œì¬, ì›ìì¬ ì´ìŠˆ ì¤‘ ìœ„ 1~4ì— ëª…í™•íˆ ì•ˆ ë§ëŠ” ê²½ìš°.\n\n"
            "ì¶œë ¥ ê·œì¹™:\n"
            "- ë°˜ë“œì‹œ ì•„ë˜ 5ê°œ ë¼ë²¨ ì¤‘ í•˜ë‚˜ë§Œ 'ê·¸ëŒ€ë¡œ' ì¶œë ¥í•œë‹¤: ê±°ì‹œê²½ì œ, í•´ì™¸ ì¦ì‹œ, êµ­ë‚´ ì‹œí™©, ì£¼ë„ ì„¹í„°, ê¸°íƒ€\n"
            "- ì¶”ê°€ ì„¤ëª…, ë¶€ì—°, ê¸°í˜¸, ë”°ì˜´í‘œ, ì¤„ë°”ê¿ˆ ì—†ì´ ë¼ë²¨ë§Œ ì¶œë ¥í•œë‹¤.\n"
        )
        safe_title = _safe_truncate(title or "", 800)
        safe_summary = _safe_truncate(final_summary or "", 2500)

        classify_user = (
            f"ë‹¤ìŒ ë‰´ìŠ¤(ì œëª©+ìš”ì•½)ë¥¼ ì½ê³ , ì¹´í…Œê³ ë¦¬ ë¼ë²¨ 1ê°œë§Œ ì„ íƒí•´ ì¶œë ¥í•˜ì„¸ìš”.\n"
            f"ì¶œë ¥ì€ ë¼ë²¨ë§Œ(ì¶”ê°€ ì„¤ëª… ê¸ˆì§€): ê±°ì‹œê²½ì œ, í•´ì™¸ ì¦ì‹œ, êµ­ë‚´ ì‹œí™©, ì£¼ë„ ì„¹í„°, ê¸°íƒ€\n\n"
            f"ìš°ì„ ìˆœìœ„ ê·œì¹™:\n"
            f"- 1ìˆœìœ„: [ë‰´ìŠ¤ ìš”ì•½] ë‚´ìš©ì„ ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ë¡œ íŒë‹¨í•œë‹¤.\n"
            f"- 2ìˆœìœ„: ìš”ì•½ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ì •ë³´ê°€ ë¶€ì¡±í•  ë•Œë§Œ [ë‰´ìŠ¤ ì œëª©]ì„ ì°¸ê³ í•œë‹¤.\n"
            f"- ì œëª©ê³¼ ìš”ì•½ì´ ì¶©ëŒí•˜ë©´ ìš”ì•½ì„ ë”°ë¥¸ë‹¤.\n\n"
            f"[ë‰´ìŠ¤ ì œëª©]\n{safe_title}\n\n"
            f"[ë‰´ìŠ¤ ìš”ì•½]\n{safe_summary}\n\n"
            f"ì¹´í…Œê³ ë¦¬:"
        )
        classify_user = _safe_truncate(classify_user, LLM_MAX_USER_CHARS)

        # ë¶„ë¥˜ ìš”ì²­
        result = call_llm_api(classify_system, classify_user, max_tokens=50)

        valid_categories = ["ê±°ì‹œê²½ì œ", "í•´ì™¸ ì¦ì‹œ", "êµ­ë‚´ ì‹œí™©", "ì£¼ë„ ì„¹í„°", "ê¸°íƒ€"]
        for cat in valid_categories:
            if cat in result:
                category = cat + "_LLM"
                break

    return final_summary, category


# ==========================================
# 6. ë‰´ìŠ¤ ë³¸ë¬¸ ì¡°íšŒ ë° êµ¬ì¡°í™”
# ==========================================

def refine_financial_structure(text: str) -> str:
    """ëŠì–´ì§„ ë¬¸ì¥ ì—°ê²° ë° ê¸ˆìœµ ê¸°í˜¸ êµ¬ì¡°í™”"""
    lines = text.splitlines()
    merged_lines = []
    bullet_pattern = re.compile(r'^[\*\-â€¢â€»\[]')
    finance_symbol_pattern = re.compile(r'^[â–²â–¼â–³â–½â†‘â†“]')
    starts_with_number = re.compile(r'^[â–²â–¼â–³â–½â†‘â†“]\s*[0-9\.]')

    for line in lines:
        line = line.strip()
        if not line: continue
        if not merged_lines:
            merged_lines.append(line)
            continue
        prev_line = merged_lines[-1]

        if bullet_pattern.match(line):
            merged_lines.append(line)
        elif finance_symbol_pattern.match(line):
            if starts_with_number.match(line):
                merged_lines[-1] += " " + line
            else:
                merged_lines.append(line)
        elif prev_line.endswith('.') or prev_line.endswith(':'):
            merged_lines.append(line)
        else:
            merged_lines[-1] += " " + line

    result = "\n".join(merged_lines)
    result = re.sub(r'(\n)([â–²â–¼â–³â–½â†‘â†“])(?!\s*[0-9])', r'\n\n\2', result)
    result = re.sub(r'[ \t]+', ' ', result)
    return result


# def get_headers(tr_cd, tr_cont="N"):
#     return {
#         "Content-Type": "application/json; charset=UTF-8",
#         "Authorization": f"Bearer {ACCESS_TOKEN}",
#         "tr_cd": tr_cd,
#         "tr_cont": tr_cont,
#         "mac_address": "00:11:22:33:44:55"
#     }


def fetch_news_body(news_id):
    """REST APIë¥¼ í†µí•´ ë‰´ìŠ¤ ìƒì„¸ ë³¸ë¬¸ ì¡°íšŒ"""
    # news_id(realkey)ê°€ ì—†ìœ¼ë©´ ìš”ì²­í•˜ì§€ ì•ŠìŒ
    if not news_id:
        print("âš ï¸ [Skip] news_id(realkey)ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return None

    # ê³µë°± ì œê±° ë“± ì „ì²˜ë¦¬
    clean_id = str(news_id).strip()

    data = {"t3102InBlock": {"sNewsno": clean_id}}

    max_retries = 3   # ìµœëŒ€ 3ë²ˆ ì¬ì‹œë„

    for attempt in range(max_retries):

        try:
            # [REFAC] ê³µí†µ REST Wrapper ì‚¬ìš© (401 ì¬ì‹œë„ ì¼ì›í™”)
            response_json = ls_post(STOCK_INVESTINFO_EP, "t3102", data, timeout=5)

            # ì‘ë‹µ ë‚´ì— ì—ëŸ¬ ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸ (API íŠ¹ì„±ì— ë”°ë¼ 200 OKë¼ë„ ë‚´ë¶€ì—ëŸ¬ì¼ ìˆ˜ ìˆìŒ)

            if "rsp_cd" in response_json and response_json["rsp_cd"] != "00000":
                print(f"âš ï¸ API ì‘ë‹µ ì—ëŸ¬: {response_json.get('rsp_msg', 'ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬')}")
                return None

            if "t3102OutBlock1" not in response_json:
                return None

            news_body = response_json["t3102OutBlock1"]
            joined_body = merge_news_bodies(news_body)
            cleaned_body = clean_base_text(joined_body)
            refined_body = refine_financial_structure(cleaned_body)

            return refined_body

        except requests.exceptions.Timeout:
            print(f"â° ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (ì‹œë„ {attempt + 1}/{max_retries})")
            time.sleep(1)
        except HTTPError as e:
            # raise_for_status()ì—ì„œ ë°œìƒí•œ HTTP ì—ëŸ¬ ì²˜ë¦¬
            status = getattr(e.response, "status_code", None)

            if status == 500:
                print(f"âš ï¸ [500 Error] ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}) - ID: {clean_id}")
                time.sleep(1)
                continue
            elif status == 401:
                # ls_post ë‚´ë¶€ì—ì„œ 1íšŒ ì¬ì‹œë„ê¹Œì§€ ìˆ˜í–‰í•˜ì§€ë§Œ,
                # í˜¹ì‹œ ëª¨ë¥¼ ì¼€ì´ìŠ¤ë¥¼ ìœ„í•´ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ë‹¤ìŒ ë£¨í”„ë¡œ ë„˜ê¹€
                print(f"ğŸ”‘ [401] ì¸ì¦ ì˜¤ë¥˜ ì§€ì† (ì‹œë„ {attempt + 1}/{max_retries}) - ID: {clean_id}")
                time.sleep(1)
                continue
        except Exception as e:
            print(f"âš ï¸ ë³¸ë¬¸ ì¡°íšŒ ì‹¤íŒ¨ ({news_id}): {e}")
            break

    # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ
    print(f"ğŸ’€ ìµœì¢… ì‹¤íŒ¨: ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ID: {clean_id})")
    return None

# ==========================================
# PDF íŒŒì¼ ë¡œë” í•¨ìˆ˜
# ==========================================
def monitor_pdf_directory(base_directory_path):
    """
    ì‹œìŠ¤í…œ ë‚ ì§œë¥¼ ìë™ìœ¼ë¡œ íŒŒì•…í•˜ì—¬ í•´ë‹¹ ë‚ ì§œ í´ë”ë¥¼ ê°ì‹œí•©ë‹ˆë‹¤.
    í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±í•˜ë©°, ìƒˆë¡œìš´ PDFë¥¼ ì²˜ë¦¬ í›„ OLD í´ë”ë¡œ ì´ë™í•©ë‹ˆë‹¤.
    """

    # ë£¨í”„ ì‹œì‘ ì „ í˜„ì¬ ë‚ ì§œ í™•ì¸
    today_str = datetime.now().strftime("%Y%m%d")
    current_watch_path = Path(base_directory_path) / today_str

    print(f"ğŸ‘€ [PDF Monitor] ì˜¤ëŠ˜ ë‚ ì§œ({today_str}) í´ë” ê°ì‹œ ì‹œì‘")
    print(f"ğŸ“ [Path] {current_watch_path}")

    # 3. ë¬´í•œ ë£¨í”„ë¡œ í´ë” ê°ì‹œ
    while True:
        try:
            # 1. ìì •ì´ ì§€ë‚˜ ë‚ ì§œê°€ ë°”ë€Œì—ˆëŠ”ì§€ ì²´í¬
            new_today_str = datetime.now().strftime("%Y%m%d")
            if new_today_str != today_str:
                today_str = new_today_str
                current_watch_path = Path(base_directory_path) / today_str
                print(f"ğŸ“… [PDF Monitor] ë‚ ì§œ ë³€ê²½ ê°ì§€: {today_str} í´ë”ë¡œ ì „í™˜")

            # 2. ê°ì‹œ ê²½ë¡œ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            if not current_watch_path.exists():
                current_watch_path.mkdir(parents=True, exist_ok=True)
                print(f"ğŸ“ [PDF Monitor] ìƒˆ ë‚ ì§œ í´ë” ìƒì„±ë¨: {current_watch_path}")

            # 3. OLD í´ë” ìƒì„±
            old_dir_path = current_watch_path / "OLD"
            old_dir_path.mkdir(parents=True, exist_ok=True)


            # í˜„ì¬ í´ë”ì— ìˆëŠ” PDF íŒŒì¼ë§Œ ê²€ìƒ‰ (iterdir() ì‚¬ìš©ìœ¼ë¡œ í•œê¸€/ê³µë°± ëŒ€ì‘ ê°•í™”)
            pdf_files = list(current_watch_path.glob("*.pdf"))

            if pdf_files:
                print(f"ğŸ” [PDF Monitor] {len(pdf_files)}ê°œì˜ ì‹ ê·œ PDF ë°œê²¬")

            for file_path in pdf_files:
                filename = file_path.name
                title = file_path.stem  # í™•ì¥ì ì œì™¸ íŒŒì¼ëª…

                # ê°œë³„ íŒŒì¼ ì²˜ë¦¬ try-except
                try:
                    # [ì¶”ê°€] ì•ˆì „í•œ ê²½ë¡œ ì²´í¬ í•¨ìˆ˜ í™œìš©
                    # í´ë”ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)

                    # date_folder = base_path.name
                    date_folder = current_watch_path.name
                    default_date = date_folder if len(date_folder) == 8 and date_folder.isdigit() else datetime.now().strftime("%Y%m%d")

                    # íŒŒì¼ ì½ê¸° ì‹œë„
                    full_text = ""
                    with pdfplumber.open(file_path) as pdf:
                        for page in pdf.pages:
                            page_text = page.extract_text()
                            if page_text:
                                full_text += page_text

                    if not full_text.strip():
                        print(f"âš ï¸ [PDF Loader] í…ìŠ¤íŠ¸ ì—†ìŒ (Skip & Move): {filename}")
                    else:
                        # íì— ë„£ì„ ë°ì´í„° êµ¬ì¡° ìƒì„±
                        news_data = {
                            "date": default_date,
                            "time": datetime.now().strftime("%H%M%S"),
                            "realkey": f"PDF_{filename}_{int(time.time())}",
                            "id": filename,
                            "title": title,
                            "body": None,
                            "full_text": full_text,
                            "source": "PDF"
                        }
                        news_queue.put(news_data)
                        print(f"ğŸ“¥ [PDF Loader] íì— ì¶”ê°€ë¨: {title}")

                    # 4. ì²˜ë¦¬ ì™„ë£Œ í›„ íŒŒì¼ ì´ë™ (shutil ì‚¬ìš© ì‹œ ê²½ë¡œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
                    destination = old_dir_path / filename

                    # ì¤‘ë³µ íŒŒì¼ëª… ì²˜ë¦¬ (íŒŒì¼ëª…_timestamp.pdf)
                    if destination.exists():
                        new_name = f"{destination.stem}_{int(time.time())}{destination.suffix}"
                        destination = old_dir_path / new_name

                    # [ì¤‘ìš”] ì´ë™ ì‹œ shutil.move(Pathê°ì²´, Pathê°ì²´)ëŠ” íŒŒì´ì¬ 3.6+ ì—ì„œ ì§€ì›
                    shutil.move(str(file_path), str(destination))
                    print(f"ğŸšš [PDF Monitor] ì´ë™ ì™„ë£Œ -> OLD/{destination.name}")

                except PermissionError:
                    print(f"ğŸ”’ [PDF Monitor] íŒŒì¼ ì‚¬ìš© ì¤‘ (Skip): {filename}")
                except Exception as e:
                    print(f"âŒ [PDF Monitor] íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({filename}): {e}")

            # 5. í´ë” ìŠ¤ìº” ì£¼ê¸° (3ì´ˆ ëŒ€ê¸°)
            time.sleep(3)

        except Exception as e:
            print(f"âš ï¸ [PDF Monitor] ë£¨í”„ ì—ëŸ¬: {e}")
            time.sleep(5)

    print("âœ… [PDF Loader] ëª¨ë“  PDF íŒŒì¼ ë¡œë”© ì™„ë£Œ")




# ==========================================
# 7. ì›Œì»¤ ìŠ¤ë ˆë“œ
# ==========================================

def worker():
    """ëŒ€ê¸°ì—´ì—ì„œ ë‰´ìŠ¤ë¥¼ êº¼ë‚´ ì²˜ë¦¬í•˜ëŠ” ì†Œë¹„ì í•¨ìˆ˜"""
    print("ğŸš€ ë‰´ìŠ¤ ì²˜ë¦¬ ì›Œì»¤(Worker) ì‹œì‘ë¨...")
    while True:
        news_item = news_queue.get()
        try:
            if news_item is None:
                print("\nğŸ›‘ ì›Œì»¤ ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ")
                return

            # [ë¶€í•˜ ì œì–´] íê°€ ë§ì´ ì°¨ìˆìœ¼ë©´ í‚¤ì›Œë“œ ì—†ëŠ” ë‰´ìŠ¤ëŠ” ìŠ¤í‚µ
            is_busy = news_queue.qsize() > (MAX_QUEUE_SIZE * 0.7)

            raw_title = news_item.get('title', 'No Title')
            title = clean_financial_text(raw_title)

            # PDFì¸ì§€ WebSocketì¸ì§€ í™•ì¸
            source = news_item.get('source', 'API')
            realkey = news_item.get('realkey')
            news_id = news_item.get('id')

            # debug (ì‚¬ìš©ì ìš”ì²­: ê¸°ì¡´ print ìœ ì§€)
            print(f"\në‚ ì§œ: {news_item.get('date')}")
            print(f"ì‹œê°„: {news_item.get('time')}")
            print(f"í‚¤ê°’: {news_item.get('realkey')}")
            print(f"ì œëª©: {news_item.get('title')}")

            raw_date = news_item.get('date')
            raw_time = news_item.get('time')
            date = f"{raw_date[:4]}-{raw_date[4:6]}-{raw_date[6:]}" if raw_date and len(raw_date) == 8 else raw_date
            time_str = f"{raw_time[:2]}:{raw_time[2:4]}:{raw_time[4:]}" if raw_time and len(raw_time) == 6 else raw_time

            # í‚¤ì›Œë“œ í•„í„°ë§ (ë¶€í•˜ê°€ ì‹¬í•  ë•Œë§Œ)
            if is_busy and not quick_keyword_classify(title):
                print(f"â© [Skip] ì‹œìŠ¤í…œ ë¶€í•˜ë¡œ ê±´ë„ˆëœ€: {title}")
                # news_queue.task_done()
                continue

            # API ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            print(f"\nğŸ”„ ì²˜ë¦¬ ì‹œì‘: {title}")
            if source == "PDF":
                # PDFëŠ” ì´ë¯¸ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§€ê³  ìˆìŒ
                raw_full_text = news_item.get('full_text', "")
                # PDF í…ìŠ¤íŠ¸ë„ ê¸°ë³¸ ì •ì œ ìˆ˜í–‰
                # merge_news_bodiesëŠ” List[Dict]ìš©ì´ë¯€ë¡œ í˜¸ì¶œí•˜ë©´ ì•ˆë¨
                # joined_full_text = merge_news_bodies(raw_full_text)
                cleaned_full_text = clean_base_text(raw_full_text)
                raw_body = refine_financial_structure(cleaned_full_text)

            else:
                time.sleep(0.5)  # 0.5ì´ˆ ì •ë„ ìˆ¨ ê³ ë¥´ê¸° í›„ ìš”ì²­

                # 1. ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                raw_body = fetch_news_body(realkey)

            if raw_body:
                print("\në‰´ìŠ¤ ë³¸ë¬¸:")
                print(raw_body + "\n")

                # 2. ìš”ì•½ + ë¶„ë¥˜ (OpenAI API ì‚¬ìš©)
                summary_body, category = summarize_and_classify(raw_body, title)

                if not summary_body or not summary_body.strip():
                    print("âš ï¸ [Fallback Summary] LLM ìš”ì•½ì´ ë¹„ì–´ìˆì–´ ì›ë¬¸ ì¼ë¶€ë¡œ ëŒ€ì²´ ì €ì¥í•©ë‹ˆë‹¤.")
                    summary_body = (raw_body[:2000] + " ...") if raw_body else "ìš”ì•½ ì‹¤íŒ¨/ì›ë¬¸ ì—†ìŒ"

                db_data = (
                    date,
                    time_str,
                    news_id,
                    realkey,
                    title,
                    len(raw_body),
                    category,
                    summary_body
                )

                # 3. DB ì €ì¥
                if True: # "ê¸°íƒ€" not in category # and "ì£¼ë„ ì„¹í„°" not in category
                    insert_to_db(db_data)
                    print(f"\nâœ… DB ì €ì¥ ì™„ë£Œ: {title} (ì¹´í…Œê³ ë¦¬: {category})")
                else:
                    print(f"\nğŸš« '{category}' ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ë˜ì–´ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {title}\n")
            else:
                print("\nâš ï¸ ë³¸ë¬¸ ì—†ìŒ, ê±´ë„ˆëœ€.")

        except Exception as e:
            print(f"\nâŒ ì›Œì»¤ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
        finally:
            news_queue.task_done()


# ==========================================
# 8. WebSocket ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
# ==========================================

def on_message(ws, message):
    try:
        response = json.loads(message)

        # body í‚¤ê°€ ìˆê³ , ê·¸ ê°’ì´ Noneì´ ì•„ë‹ ë•Œë§Œ ì²˜ë¦¬
        if "body" in response and response["body"] is not None:
            news_data = response["body"]

            # [ì¶”ê°€] í•„ìˆ˜ ë°ì´í„°(ì œëª© ë“±)ê°€ ìˆëŠ”ì§€ í•œ ë²ˆ ë” í™•ì¸ (ì•ˆì „ì¥ì¹˜)
            if not isinstance(news_data, dict): return

            try:
                # íê°€ ê½‰ ì°¨ë©´ ì˜ˆì™¸ ë°œìƒ (ë°ì´í„° ë²„ë¦¼)
                news_queue.put(news_data, block=False)

                # news_queue.put(news_data)
                print(f"ğŸ“© [ìˆ˜ì‹ ] {news_data.get('title', 'ì œëª©ì—†ìŒ')} -> ëŒ€ê¸°ì—´ ì¶”ê°€ë¨")
            except queue.Full:
                print(f"ğŸ”¥ [Drop] í ê°€ë“ ì°¸! ë‰´ìŠ¤ ë²„ë¦¼: {news_data.get('title')}")
        else:
            # bodyê°€ ì—†ê±°ë‚˜ Noneì¸ ê²½ìš° (Heartbeatë‚˜ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì¼ ìˆ˜ ìˆìŒ)
            pass


    except Exception as e:
        print(f"ë©”ì‹œì§€ íŒŒì‹± ì—ëŸ¬: {e}")


# [ìˆ˜ì • ì „]
# def on_open(ws):
#     print("ğŸŒ WebSocket ì—°ê²° ë° êµ¬ë… ìš”ì²­")
#     sub_msg = {
#         "header": {"token": ACCESS_TOKEN, "tr_type": "3"},  <-- ì—¬ê¸°ì„œ ACCESS_TOKENì€ Noneì…ë‹ˆë‹¤.
#         "body": {"tr_cd": "NWS", "tr_key": "NWS001"}
#     }
#     ws.send(json.dumps(sub_msg))

# [ìˆ˜ì • í›„]
def on_open(ws):
    print("ğŸŒ WebSocket ì—°ê²° ë° êµ¬ë… ìš”ì²­")

    # 1. ìœ íš¨í•œ í† í°ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (ë§Œë£Œë˜ì—ˆìœ¼ë©´ ìë™ ê°±ì‹ ë¨)
    token = issue_ls_access_token(force=False)

    if not token:
        print("âŒ [Error] ì›¹ì†Œì¼“ êµ¬ë… ì‹¤íŒ¨: í† í° ë°œê¸‰ ë¶ˆê°€")
        return

    sub_msg = {
        "header": {"token": token, "tr_type": "3"},  # 2. ë°œê¸‰ë°›ì€ token ë³€ìˆ˜ ì‚¬ìš©
        "body": {"tr_cd": "NWS", "tr_key": "NWS001"}
    }
    ws.send(json.dumps(sub_msg))


# ==========================================
# 9. ë©”ì¸ ì‹¤í–‰ë¶€
# ==========================================

if __name__ == "__main__":
    # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
    # ì›Œì»¤ ìˆ˜ ì„¤ì • (ë¡œì»¬ LLM ì‚¬ìš© ì‹œ 1~2ê°œê°€ ì ì ˆ)
    num_workers = 1
    print(f"ğŸ§µ ì›Œì»¤ ìŠ¤ë ˆë“œ ìˆ˜: {num_workers}")
    # num_workers = min(3, max(1, n_cpu_cores - 2))

    # print(f"ğŸ§µ ì›Œì»¤ ìŠ¤ë ˆë“œ ìˆ˜: {num_workers}")

    worker_threads = []
    for _ in range(num_workers):
        t = threading.Thread(target=worker, daemon=True)
        t.start()
        worker_threads.append(t)

    # PDF íŒŒì¼ ë¡œë” ì‹¤í–‰ (ë³„ë„ ìŠ¤ë ˆë“œ í˜¹ì€ ì‹œì‘ ì „ ì‹¤í–‰)
    # WebSocketê³¼ ë™ì‹œì— ëŒë¦¬ë ¤ë©´ ìŠ¤ë ˆë“œë¡œ,
    pdf_thread = threading.Thread(target=monitor_pdf_directory, args=(PDF_BASE_PATH,), daemon=True)
    pdf_thread.start()

    # ì›¹ì†Œì¼“ ì‹¤í–‰ (ë¼ì´ë¸Œ ë‰´ìŠ¤ë„ ê³„ì† ë°›ê³  ì‹¶ë‹¤ë©´ ìœ ì§€)
    # ì›¹ì†Œì¼“ ë””ë²„ê·¸ ë¡œê·¸ ë„ê¸°
    websocket.enableTrace(False)

    ws_app = websocket.WebSocketApp(
        WS_URL,
        on_message=on_message,
        on_open=on_open,
        on_close=lambda ws, status_cd, msg: print("WebSocket ì—°ê²° ì¢…ë£Œ:", status_cd, msg),
        on_error=lambda ws, error: print("WebSocket ì—ëŸ¬:", error)
    )

    try:
        print("\nğŸ“¡ WebSocket í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰ ì¤‘...")
        ws_app.run_forever()

    except KeyboardInterrupt:
        print("í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì¤‘...")
        for _ in worker_threads:
            news_queue.put(None)
        for t in worker_threads:
            t.join(timeout=2.0) # ìŠ¤ë ˆë“œê°€ ì¢…ë£Œë  ë•Œê¹Œì§€ ìµœëŒ€ 2ì´ˆë§Œ ê¸°ë‹¤ë¦¼.
